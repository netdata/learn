---
custom_edit_url: "https://github.com/netdata/netdata/edit/master/src/collectors/guides/proxmox/metadata.yaml"
sidebar_label: "Proxmox VE Monitoring"
learn_status: "Published"
toc_max_heading_level: "4"
learn_rel_path: "Collecting Metrics/Containers and Orchestration"
keywords: [proxmox, proxmox ve, pve, kvm, qemu, lxc, virtualization, hypervisor, virtual machines, containers, ceph, zfs, corosync, cluster]
message: "DO NOT EDIT THIS FILE DIRECTLY, IT IS GENERATED BY THE COLLECTOR'S metadata.yaml FILE"
most_popular: "True"
sidebar_position: "250"
learn_link: "https://learn.netdata.cloud/docs/collecting-metrics/containers-and-orchestration/proxmox-ve-monitoring"
slug: "/collecting-metrics/containers-and-orchestration/proxmox-ve-monitoring"
---


# Proxmox VE Monitoring


<img src="https://netdata.cloud/img/proxmox.png" width="150"/>


Plugin: guides
Module: proxmox

<img src="https://img.shields.io/badge/maintained%20by-Netdata-%2300ab44" />

## Overview

This guide describes how Netdata monitors Proxmox VE hypervisors. Netdata provides comprehensive, zero-configuration monitoring of Proxmox hosts, including per-VM and per-container resource utilization, host system metrics, storage health, and cluster components.

When installed on a Proxmox host, Netdata automatically discovers and monitors all KVM/QEMU virtual machines and LXC containers through Linux cgroups, resolving friendly names for each VM and container.


Netdata uses multiple collectors working together to provide full Proxmox visibility:

- **cgroups.plugin** monitors per-VM and per-container CPU, memory, disk I/O, and network via Linux cgroups. It automatically resolves VM names from `/etc/pve/qemu-server/<VMID>.conf` and container hostnames from `/etc/pve/lxc/<CTID>.conf`.
- **proc.plugin** monitors host-level system metrics (CPU, memory, network interfaces, disk I/O).
- **apps.plugin** monitors Proxmox-specific process groups (`proxmox-ve`, `libvirt`, `qemu-guest-agent`).
- **go.d/zfspool** monitors ZFS pool health, space utilization, and fragmentation (ZFS is common on Proxmox).
- **go.d/ceph** monitors Ceph cluster health and performance (for Proxmox clusters using Ceph storage).
- **go.d/smartctl** monitors physical disk SMART health data.
- **go.d/sensors** monitors hardware temperature, fan speed, and voltage.
- **ebpf.plugin** provides kernel-level visibility into VM/container syscalls, file I/O, and network activity.


This collector is only supported on the following platforms:

- Linux

This collector only supports collecting metrics from a single instance of this integration.


Proxmox VE Monitoring can be monitored further using the following other integrations:

- [Proxmox VMs and Containers](/docs/collecting-metrics/containers-and-orchestration/proxmox-vms-and-containers)
- [Systemd Services](/docs/collecting-metrics/operating-systems/systemd-services)
- [Applications](/docs/collecting-metrics/operating-systems/applications)
- [ZFS Pools](/docs/collecting-metrics/storage/zfs-pools)
- [Ceph](/docs/collecting-metrics/storage/ceph)
- [S.M.A.R.T.](/docs/collecting-metrics/hardware-and-iot/s.m.a.r.t.)
- [Linux Sensors](/docs/collecting-metrics/hardware-and-iot/linux-sensors)
- [Network interfaces](/docs/collecting-metrics/operating-systems/network-interfaces)
- [Disk Statistics](/docs/collecting-metrics/operating-systems/disk-statistics)
- [System statistics](/docs/collecting-metrics/operating-systems/system-statistics)
- [Memory Usage](/docs/collecting-metrics/operating-systems/memory-usage)
- [ZFS Adaptive Replacement Cache](/docs/collecting-metrics/operating-systems/zfs-adaptive-replacement-cache)

### Default Behavior

#### Auto-Detection

When Netdata is installed on a Proxmox VE host, it automatically detects and monitors:

- All running KVM/QEMU virtual machines
- All running LXC containers
- Host system resources (CPU, memory, network, disks)
- Systemd services (pveproxy, pvedaemon, pvestatd, corosync, etc.)
- ZFS pools (if ZFS is used)


#### Limits

The default configuration for this integration does not impose any limits on data collection.

#### Performance Impact

The default configuration for this integration is not expected to impose a significant performance impact on the system.

## Metrics

This guide does not collect metrics directly. Metrics are collected by the related integrations listed above. See each integration's page for detailed metric documentation.



## Alerts

There are no alerts configured by default for this integration.


## Setup


### Prerequisites

#### Install Netdata on the Proxmox host

Netdata must be installed directly on the Proxmox VE host (not inside a VM or container) to access cgroups for all VMs and containers.

```bash
wget -O /tmp/netdata-kickstart.sh https://get.netdata.cloud/kickstart.sh && sh /tmp/netdata-kickstart.sh
```



### Configuration

#### Options



There are no configuration options.



#### via File

There is no configuration file.

##### Examples
There are no configuration examples.



## Troubleshooting

### VM or container names not resolved

If VMs or containers show raw cgroup paths instead of friendly names, verify that:

1. Netdata is installed on the Proxmox host (not inside a VM)
2. The `/etc/pve/` directory is accessible to the netdata user
3. The `cgroup-name.sh` script can read VM/container configuration files


### Missing ZFS metrics

If ZFS pool metrics are not showing, ensure the `zfspool` collector is enabled and the `zpool` command is available to the netdata user.


### Missing Ceph metrics

Ceph metrics require the Ceph collector to be configured with the Ceph REST API endpoint. See the Ceph integration page for details.



