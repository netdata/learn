"""
This is the script that gathers markdown files from all of Netdata's repos in this repo

Stages of this ingest script:

    Stage_1: Ingest every available markdown from the defaultRepos

    Stage_2: We create three buckets:
                1. all_markdown_files: all the markdown files in defaultRepos
                2. markdown_files_with_metadata: all the markdown files that have hidden metadata fields
                3. toPublish: markdown files that must be included in the learn 
                    (metadata_key_value: "learn_status": "Published") 

    Stage_3: 
        1. Move the toPublish markdown files under the DOCS_PREFIX folder based on their metadata (they decide where, 
            they live)
        2. Generate autogenerated pages

    Stage_4: Sanitization
                1. Make the hidden metadata fields actual readable metadata for docusaurus
                2. 
                
    Stage_5: Convert GH links to version specific links
"""

# Imports
import argparse
import ast
import errno
import glob
import os
import re
import shutil
import urllib.parse
from pathlib import Path
import json
import git
import numpy as np
import pandas as pd

import autogenerateRedirects as genRedirects

DRY_RUN = False
DEBUG = False
DOCS_PREFIX = "will be added by arguments"

rest_files_dictionary = {}
rest_files_with_metadata_dictionary = {}
to_publish = {}
all_markdown_files = []
UNCORRELATED_LINK_COUNTER = 0
# Dict mapping repo name to dict of broken URLs -> set of source files
UNCORRELATED_URLS_BY_REPO = {}
# Dict mapping repo name to dict of (url, header) -> set of source files
BROKEN_HEADER_LINKS_BY_REPO = {}
FAIL_ON_REPOS = set()  # Set of repo names to fail on if broken links found
FAIL_ON_ALL_BROKEN_LINKS = False  # If True, fail on any broken link regardless of repo
IGNORE_ON_PREM_REPO = False  # If True, skip cloning on-prem repo and ignore links pointing to it
LOCAL_REPOS = {}  # Dict mapping repo name to local path (use local dir instead of cloning)
# Temporarily until we release (change it (the default) to /docs
# version_prefix = "nightly"  # We use this as the version prefix in the link strategy
TEMP_FOLDER = "ingest-temp-folder"
default_repos = {
    "netdata":
        {
            "owner": "netdata",
            "branch": "master",
            "HEAD": "master",
        },
    "netdata-cloud-onprem":
        {
            "owner": "netdata",
            "branch": "master",
            "HEAD": "master",
        },
    ".github":
        {
            "owner": "netdata",
            "branch": "main",
            "HEAD": "main",
        },
    "agent-service-discovery":
        {
            "owner": "netdata",
            "branch": "master",
            "HEAD": "master",
        },
    "netdata-grafana-datasource-plugin":
        {
            "owner": "netdata",
            "branch": "master",
            "HEAD": "master",
        },
    "helmchart":
        {
            "owner": "netdata",
            "branch": "master",
            "HEAD": "master",
        }
}


POS_RE = re.compile(r'(?m)^\s*sidebar_position\s*:\s*"?(\d+)"?\s*$')

def ensure_category_json_for_dirs(docs_root):
    """
    For every directory under docs_root:
    - if <dir>/<basename(dir)>.mdx exists => treat as category overview; do nothing
    - else create or overwrite _category_.json using min sidebar_position of direct-child mdx files
    """
    for dirpath, dirnames, filenames in os.walk(docs_root):
        abs_dir = os.path.abspath(dirpath)
        abs_root = os.path.abspath(docs_root)

        base = os.path.basename(os.path.normpath(dirpath))

        # 0) skip docs root
        if abs_dir == abs_root:
            continue

        category_json = os.path.join(dirpath, "_category_.json")
        
        if os.path.exists(os.path.join(dirpath, f"{base}.mdx")):
            continue

        # 4) compute position from direct child mdx files
        mdx_files = [f for f in filenames if f.lower().endswith(".mdx")]

        positions = []
        for fn in mdx_files:
            p = os.path.join(dirpath, fn)
            try:
                with open(p, "r", encoding="utf-8") as fh:
                    head = fh.read(64 * 1024)
            except OSError:
                continue

            m = POS_RE.search(head)
            if m:
                positions.append(int(m.group(1)))

        if positions:
            cat_pos = min(positions)
        else:
            if not mdx_files:
                continue

            mdx_files.sort(key=lambda s: s.casefold())
            cat_pos = 999999999  # choose your fallback policy

        payload = {"label": base, "position": cat_pos, "link": None}

        with open(category_json, "w", encoding="utf-8") as f:
            json.dump(payload, f, ensure_ascii=False, indent=2)
            f.write("\n")

        print(f"CREATE {category_json} with position={cat_pos} label='{base}'")


def clean_and_lower_string(string):
    return re.sub(r'(-)+', '-', string.lower().replace(",", "-").replace(" ", "-").replace("//", "/"))


def extract_headers_from_file(file_path):
    """
    Extract all headers from a markdown file and return them as a set of anchor IDs.
    Headers are converted to anchor format: lowercase, spaces to hyphens, special chars removed.
    """
    headers = set()
    try:
        content = Path(file_path).read_text()
        # Match markdown headers (# Header, ## Header, etc.)
        header_pattern = r'^#{1,6}\s+(.+)$'
        for match in re.finditer(header_pattern, content, re.MULTILINE):
            header_text = match.group(1).strip()
            # Convert header to anchor ID (similar to how markdown processors do it)
            # Remove inline code backticks, bold/italic markers
            anchor = re.sub(r'[`*_]', '', header_text)
            # Remove HTML tags
            anchor = re.sub(r'<[^>]+>', '', anchor)
            # Convert to lowercase, replace spaces with hyphens
            anchor = anchor.lower().replace(' ', '-')
            # Remove special characters except hyphens
            anchor = re.sub(r'[^a-z0-9-]', '', anchor)
            # Remove multiple consecutive hyphens
            anchor = re.sub(r'-+', '-', anchor)
            # Remove leading/trailing hyphens
            anchor = anchor.strip('-')
            if anchor:
                headers.add(anchor)
    except Exception as e:
        pass
    return headers


def validate_header_in_file(file_path, header):
    """
    Check if a header/anchor exists in the target file.
    Returns True if the header exists or if header is empty, False otherwise.
    """
    if not header:
        return True
    headers = extract_headers_from_file(file_path)
    # Also check the raw header (some anchors are preserved as-is)
    return header.lower() in headers or header in headers


def extract_repo_from_github_url(url):
    """
    Extract the repository name from a GitHub URL.
    
    Example:
        https://github.com/netdata/netdata/blob/master/src/file.md -> netdata
        https://github.com/netdata/helmchart/blob/master/README.md -> helmchart
    """
    if not url.startswith("https://github.com/netdata/"):
        return "unknown"

    # URL format: https://github.com/netdata/<repo>/...
    parts = url.replace("https://github.com/netdata/", "").split("/")
    if parts:
        return parts[0]
    return "unknown"


def extract_repo_from_local_path(path):
    """
    Extract the repository name from a local path in the temp folder.
    
    Example:
        ingest-temp-folder/netdata/src/file.md -> netdata
        docs/something/file.mdx -> unknown (not from temp folder)
    """
    if path.startswith(TEMP_FOLDER + "/"):
        parts = path.replace(TEMP_FOLDER + "/", "").split("/")
        if parts:
            return parts[0]
    return "unknown"


def add_broken_url(repo, url, source_file):
    """Add a broken URL to the tracking dictionary, categorized by repo."""
    global UNCORRELATED_URLS_BY_REPO

    # If ignoring on-prem repo, skip URLs that point to it
    if IGNORE_ON_PREM_REPO:
        target_repo = extract_repo_from_github_url(url)
        if target_repo == "netdata-cloud-onprem":
            return

    if repo not in UNCORRELATED_URLS_BY_REPO:
        UNCORRELATED_URLS_BY_REPO[repo] = {}
    if url not in UNCORRELATED_URLS_BY_REPO[repo]:
        UNCORRELATED_URLS_BY_REPO[repo][url] = set()
    UNCORRELATED_URLS_BY_REPO[repo][url].add(source_file)


def add_broken_header(repo, full_link, header, source_file):
    """Add a broken header link to the tracking dictionary, categorized by repo."""
    global BROKEN_HEADER_LINKS_BY_REPO

    # If ignoring on-prem repo, skip links that point to it
    if IGNORE_ON_PREM_REPO:
        target_repo = extract_repo_from_github_url(full_link)
        if target_repo == "netdata-cloud-onprem":
            return

    if repo not in BROKEN_HEADER_LINKS_BY_REPO:
        BROKEN_HEADER_LINKS_BY_REPO[repo] = {}
    key = (full_link, header)
    if key not in BROKEN_HEADER_LINKS_BY_REPO[repo]:
        BROKEN_HEADER_LINKS_BY_REPO[repo][key] = set()
    BROKEN_HEADER_LINKS_BY_REPO[repo][key].add(source_file)


def github_url_to_local_path(url):
    """
    Convert a GitHub URL to a local path in the temp folder.
    Returns the local path if it can be constructed, None otherwise.
    
    Example:
        https://github.com/netdata/netdata/blob/master/src/go/pkg/prometheus/selector/README.md
        -> ingest-temp-folder/netdata/src/go/pkg/prometheus/selector/README.md
    """
    if not url.startswith("https://github.com/netdata"):
        return None

    # Convert URL to local path
    local_path = url.replace("https://github.com/netdata", TEMP_FOLDER)
    local_path = local_path.replace("edit/", "blob/", 1)
    local_path = local_path.replace("blob/master/", "")
    local_path = local_path.replace("blob/main/", "")

    return local_path


def file_exists_in_repos(url):
    """
    Check if a GitHub URL points to a file that exists in the cloned repos.
    Returns True if the file exists, False otherwise.
    """
    local_path = github_url_to_local_path(url)
    if local_path is None:
        return False
    return Path(local_path).exists()


def convert_parenthetical_slash(segment: str) -> str:
    """
    Convert occurrences like "(ABC/XYZ)" into "ABC-XYZ" inside a string.
    Only converts simple parenthetical groups containing a single slash.
    """
    if not segment:
        return segment

    # Replace occurrences of (A/B) or (A/B/C) with A-B or A-B-C respectively
    def repl(m):
        inner = m.group(1)
        parts = inner.split('/')
        return '-'.join(parts)

    return re.sub(r"\(([^()]+?/[^()]+?)\)", repl, segment)


def populate_integrations(markdownFiles):
    """
    if a symlink, read that, if not, look inside integrations folder.
    """

    print("### Populating map from Integration metadata rows ###\n")

    metadata_dictionary = {}
    ignore_dup = []

    # Read the map file, to replace the placeholder for the dynamic part
    map_file = pd.read_csv("map.csv")

    collectors_entries = pd.DataFrame()
    exporting_entries = pd.DataFrame()
    alerting_agent_entries = pd.DataFrame()
    alerting_cloud_entries = pd.DataFrame()
    authentication_entries = pd.DataFrame()
    logs_entries = pd.DataFrame()

    readmes_first = []
    others_last = []
    for file in markdownFiles:
        if "README.md" in file:
            readmes_first.append(file)
        else:
            others_last.append(file)

    markdownFiles = readmes_first + others_last

    for file in markdownFiles:
        path = file.split("integrations")[0].replace("README.md", "")

        whole_file = Path(file).read_text()

        if whole_file not in ignore_dup and "DO NOT EDIT THIS FILE DIRECTLY" in whole_file:

            meta = whole_file.split(
                "endmeta-->")[0].replace("<!--startmeta", "---") + "---"

            metadata_dictionary = read_metadata(meta)

            if os.path.islink(file):
                ignore_dup.append(whole_file)
                # If it is a manual symlink, meaning a README symlink but the folder has more than one integration, thus their custom_edit_urls are unique. 1:1 integrations have the README link as custom_edit_url
                if not file.replace("ingest-temp-folder/", "").split('/', 1)[1] in metadata_dictionary[
                    'custom_edit_url']:
                    proper_edit_url = file.replace(
                        "ingest-temp-folder/", "")

                    proper_edit_url = "https://github.com/netdata/" + \
                                      proper_edit_url.split(
                                          '/', 1)[0] + "/edit/master/" + proper_edit_url.split('/', 1)[1]
                    metadata_dictionary['custom_edit_url'] = proper_edit_url

                    # print("path:", file)
                    # print(metadata_dictionary)

            metadf = pd.DataFrame([metadata_dictionary])
            # print(file)
            if "collector" in path:
                collectors_entries = pd.concat(
                    [collectors_entries, metadf])
                # print(collectors_entries)
                # quit()
            elif "exporting" in path:
                exporting_entries = pd.concat([exporting_entries, metadf])
                # print(exporting_entries)
            elif "cloud-authentication" in file:
                authentication_entries = pd.concat([authentication_entries, metadf])
            # here we need a different check, as the path variable gets messed up
            elif "cloud-notifications" in file:

                # print("in")
                alerting_cloud_entries = pd.concat(
                    [alerting_cloud_entries, metadf])
            elif "logs" in file:
                # Custom location for Logs integrations, as they normally have a pretty big README that we add as a reference, as a child to the integration's folder.
                metadf['learn_rel_path'] = metadf['learn_rel_path'] + "/" + metadf['sidebar_label']

                logs_entries = pd.concat([logs_entries, metadf])
            else:
                alerting_agent_entries = pd.concat(
                    [alerting_agent_entries, metadf])

    # print("Collectors\n", collectors_entries, "Agent alerts\n", alerting_agent, "Cloud alerts\n",  alerting_cloud, "Exporting",  exporting_entries)

    replace_index = map_file.loc[map_file['custom_edit_url']
                                 == "authentication_integrations"].index
    # print(replace_index[0])
    upper = map_file.iloc[:replace_index[0]]
    lower = map_file.iloc[replace_index[0] + 1:]

    map_file = pd.concat([upper, authentication_entries.sort_values(
        by=['learn_rel_path', 'sidebar_label'], key=lambda col: col.str.lower()), lower], ignore_index=True)

    replace_index = map_file.loc[map_file['custom_edit_url']
                                 == "collectors_integrations"].index
    # print(replace_index[0])
    upper = map_file.iloc[:replace_index[0]]
    lower = map_file.iloc[replace_index[0] + 1:]

    map_file = pd.concat([upper, collectors_entries.sort_values(
        by=['learn_rel_path', 'sidebar_label'], key=lambda col: col.str.lower()), lower], ignore_index=True)

    replace_index = map_file.loc[map_file['custom_edit_url']
                                 == "agent_notifications_integrations"].index
    # print(replace_index[0])
    upper = map_file.iloc[:replace_index[0]]
    lower = map_file.iloc[replace_index[0] + 1:]

    map_file = pd.concat([upper, alerting_agent_entries.sort_values(
        by=['learn_rel_path', 'sidebar_label'], key=lambda col: col.str.lower()), lower], ignore_index=True)

    replace_index = map_file.loc[map_file['custom_edit_url']
                                 == "cloud_notifications_integrations"].index
    upper = map_file.iloc[:replace_index[0]]
    lower = map_file.iloc[replace_index[0] + 1:]

    map_file = pd.concat([upper, alerting_cloud_entries.sort_values(
        by=['learn_rel_path', 'sidebar_label'], key=lambda col: col.str.lower()), lower], ignore_index=True)

    replace_index = map_file.loc[map_file['custom_edit_url']
                                 == "exporters_integrations"].index
    # print(replace_index[0])
    upper = map_file.iloc[:replace_index[0]]
    lower = map_file.iloc[replace_index[0] + 1:]

    map_file = pd.concat([upper, exporting_entries.sort_values(
        by=['learn_rel_path', 'sidebar_label'], key=lambda col: col.str.lower()), lower], ignore_index=True)

    replace_index = map_file.loc[map_file['custom_edit_url']
                                 == "logs_integrations"].index
    upper = map_file.iloc[:replace_index[0]]
    lower = map_file.iloc[replace_index[0] + 1:]

    map_file = pd.concat([upper, logs_entries.sort_values(
        by=['learn_rel_path', 'sidebar_label'], key=lambda col: col.str.lower()), lower], ignore_index=True)

    map_file.to_csv("ingest/generated_map.tsv", sep='\t', index=False)

    # quit()
    return map_file


def unsafe_cleanup_folders(folder_to_delete):
    """Cleanup every file in the specified folderToDelete."""
    print("Try to clean up the folder: ", folder_to_delete)
    try:
        shutil.rmtree(folder_to_delete)
        print("Done")
    except Exception as e:
        print("Couldn't delete the folder due to the exception: \n", e)


def produce_gh_view_link_for_repo(repo, file_path):
    """
    This function return the GitHub link (view link) of a repo e.g <owner>/<repo>
    Limitation it produces only  the master, main links only for the netdata org
    """
    if repo == ".github":
        return f"https://github.com/netdata/{repo}/blob/main/{file_path}"
    else:
        return f"https://github.com/netdata/{repo}/blob/master/{file_path}"


def produce_gh_edit_link_for_repo(repo, file_path):
    """
    This function return the GitHub link (view link) of a repo e.g <owner>/<repo>
    Limitation it produces only  the master, main links only for the netdata org
    """
    if repo == ".github":
        return f"https://github.com/netdata/{repo}/edit/main/{file_path}"
    else:
        return "https://github.com/netdata/{repo}/edit/master/{file_path}"


def safe_cleanup_learn_folders(folder_to_delete):
    """
    Cleanup every file in the specified folderToDelete, that doesn't have the `part_of_learn: True`
    metadata in its metadata. It also prints a list of the files that don't have this kind of
    """
    deleted_files = []
    md_files = fetch_markdown_from_repo(folder_to_delete)
    json_files = fetch_json_from_repo(folder_to_delete)
    print(
        f"Files in the {folder_to_delete} folder #{len(md_files) + len(json_files)} which are about to be deleted")
    for md in md_files:

        metadata = read_metadata(Path(md).read_text().split("-->")[0])
        try:
            if "part_of_learn" in metadata.keys():
                # Reductant condition to emphasize what we are looking for when we clean up learn files
                if metadata["part_of_learn"] == "True":
                    pass
            else:
                deleted_files.append(md)
                os.remove(md)
        except Exception as e:
            print(f"Couldn't delete the {md} file reason: {e}")
    for json in json_files:
        deleted_files.append(json)
        os.remove(json)
    print(
        f"Cleaned up #{len(deleted_files)} files under {folder_to_delete} folder")


def verify_string_is_dictionary(string_input):
    """
    function to verify that a string input is of dictionary type
    """
    try:
        if isinstance(ast.literal_eval(string_input), dict):
            return True
        else:
            return False
    except:
        return False


def unpack_dictionary_string_to_dictionary(string_input):
    return ast.literal_eval(string_input)


def copy_doc(src, dest):
    """
    Copy a file
    """
    # Get the path
    try:
        shutil.copy(src, dest)
    except IOError as e:
        # ENOENT(2): file does not exist, raised also on missing dest parent dir
        if e.errno != errno.ENOENT:
            raise
        # try creating parent directories
        os.makedirs(os.path.dirname(dest))
        shutil.copy(src, dest)


def clone_repo(owner, repo, branch, depth, prefix_folder, gh_token=None, use_plain_https=False):
    """
    Clone a repo using:
    - HTTPS + token (if gh_token is provided),
    - HTTPS without token (if use_plain_https=True),
    - SSH otherwise.

    After cloning, the remote URL is scrubbed to a clean HTTPS URL.
    """
    try:
        output_folder = os.path.join(prefix_folder, repo)

        if use_plain_https:
            # Forced plain HTTPS (no token)
            gh_url = f"https://github.com/{owner}/{repo}.git"
        elif gh_token:
            # HTTPS + PAT
            enc = urllib.parse.quote(gh_token, safe="")
            gh_url = f"https://x-access-token:{enc}@github.com/{owner}/{repo}.git"
        else:
            # SSH fallback
            gh_url = f"git@github.com:{owner}/{repo}.git"

        git.Git().clone(gh_url, output_folder, depth=depth, branch=branch)

        # Scrub remote to clean HTTPS
        clean_url = f"https://github.com/{owner}/{repo}.git"
        git.Git(output_folder).remote('set-url', 'origin', clean_url)

        return f"Cloned the {branch} branch from {repo} repo (owner: {owner})"
    except Exception as e:
        return (f"Couldn't clone the {branch} branch from {repo} repo (owner: {owner}) \n"
                f" Exception {e} raised")


def create_mdx_path_from_metadata(metadata):
    """
    Create a path from the documents metadata
    REQUIRED KEYS in the metadata input:
        [sidebar_label, learn_rel_path]
    In the returned (final) path we sanitize "/", "//" , "-", "," with one dash
    """
    final_file = ' '.join((metadata["sidebar_label"]
                           .replace("'", " ")
                           .replace(":", " ")
                           .replace("/", " ")
                           .replace(")", " ")
                           .replace(",", " ")
                           .replace("(", " ")
                           .replace("`", " ")).split())
    
    slug = "/{}/{}".format(metadata["learn_rel_path"],
                                    final_file.replace(" ", "-")).lower().replace("//", "/")
    

    if slug.rsplit("/")[-1] == slug.rsplit("/")[-2]:
        return [
            "{}/{}/{}.mdx".format(
                DOCS_PREFIX,
                metadata["learn_rel_path"],
                final_file
            ).replace("//", "/"),
            re.sub('//+', '/', "/{}/{}".format(
                metadata["learn_rel_path"],
                final_file.replace(" ", "-")).lower().rsplit("/",1)[0]
                )
            
            ]
    else:
        return [
            "{}/{}/{}.mdx".format(
                DOCS_PREFIX,
                metadata["learn_rel_path"],
                final_file
            ).replace("//", "/"),
            re.sub('//+', '/', "/{}/{}".format(
                metadata["learn_rel_path"],
                final_file.replace(" ", "-")).lower()
                )
        ]


def fetch_markdown_from_repo(output_folder):
    return glob.glob(
        output_folder + '/**/*.md*', recursive=True) + glob.glob(output_folder + '/.**/*.md*', recursive=True)

def fetch_json_from_repo(output_folder):
    return glob.glob(
        output_folder + '/**/*.json*', recursive=True) + glob.glob(output_folder + '/.**/*.json*', recursive=True)


def insert_and_read_hidden_metadata_from_doc(path_to_file, dictionary):
    """
    Taking a path of a file as input
    Identify the area with pattern " <!-- ...multiline string -->" and  converts them
    to a dictionary of key:value pairs
    """
    # TODO work here, predict yaml file from path, should be easy, if readme try os.exists for meta yaml, if inside integrations folder, try one out.

    # TODO unique in custom edit url might need custom editurl + sidebar_label so it can be reproduced here.
    repo, path = path_to_file.replace("ingest-temp-folder/", "").split('/', 1)

    if repo == ".github":
        key = "https://github.com/netdata/" + repo + "/edit/main" + "/" + path
    else:
        key = "https://github.com/netdata/" + repo + "/edit/master" + "/" + path

    output = ""
    for field in dictionary.loc[dictionary['custom_edit_url'] == key]:
        try:
            val = dictionary.loc[dictionary['custom_edit_url']
                                 == key][field].values[0]

            # print((not val == np.nan),  val != val, val)
            val = str(val)

            if (not val == np.nan) and val != "nan":

                if field == "learn_rel_path":
                    if val == "root":
                        # print("ROOT")
                        val = "/"

                    if "Collecting Metrics" in val or "Collecting Metrics" in val:
                        output += "toc_max_heading_level: 4\n"

                if field == "sidebar_position":
                    output += "{0}: \"{1}\"\n".format(field,
                                                      val.replace("\"", ""))
                else:
                    output += "{0}: \"{1}\"\n".format(field,
                                                      val.replace("\"", ""))

        except Exception as e:
            pass

    if len(output) > 0:
        output = "<!--\n" + output + "-->\n"

        whole_file = Path(path_to_file).read_text()

        if whole_file.startswith("<!--"):
            body = whole_file.split("-->", 1)[1]
        else:
            body = whole_file

        Path(path_to_file).write_text(output + body)

        # print(path_to_file, output)
        metadata_dictionary = {}
        with open(path_to_file, "r+") as fd:
            raw_text = "".join(fd.readlines())
            pattern = r"((^<!--|^---)\n)((.|\n)*?)(\n(-->|---))"
            match_group = re.search(pattern, raw_text)
            # print(match_group)
            if match_group:
                raw_metadata = match_group[3]
                list_metadata = raw_metadata.split("\n")
                while list_metadata:
                    line = list_metadata.pop(0)
                    split_in_keywords = line.split(": ", 1)
                    key = split_in_keywords[0]
                    value = split_in_keywords[1]
                    if verify_string_is_dictionary(value):
                        value = unpack_dictionary_string_to_dictionary(value)
                    # If it's a multiline string
                    while list_metadata and len(list_metadata[0].split(": ", 1)) <= 1:
                        line = list_metadata.pop(0)
                        value = value + line.lstrip(' ')
                    value = value.strip("\"")
                    metadata_dictionary[key] = value.lstrip('>-')
        # print("\n\n")
        return metadata_dictionary
    else:
        return []


def update_metadata_of_file(path_to_file, dictionary):
    """
    Taking a path of a file as input
    Identify the area with pattern
    "<!-- ...multiline string -->"
    and converts them to a dictionary
    of key:value pairs
    """

    output = ""

    for field in dictionary:
        val = dictionary[field]
        # Special-case keywords: accept CSV values (strings with quotes/brackets)
        # and also Python lists. Normalize to an inline YAML array.
        if field == 'keywords':
            # If it's already a list, use it directly
            if isinstance(val, (list, tuple)):
                items = [str(v).strip().strip('"').strip("'") for v in val if str(v).strip()]
            else:
                # Normalize to string and strip outer quotes
                sval = str(val).strip()
                # Remove surrounding double quotes if present
                if sval.startswith('"') and sval.endswith('"'):
                    sval = sval[1:-1]
                if sval.startswith("'") and sval.endswith("'"):
                    sval = sval[1:-1]
                # If value is in bracket form [a,b,...], strip brackets
                if sval.startswith('[') and sval.endswith(']'):
                    sval_inner = sval[1:-1]
                else:
                    sval_inner = sval
                # Split on commas to form array items
                items = [it.strip().strip('"').strip("'") for it in sval_inner.split(',') if it.strip()]
            # Join items without surrounding quotes to match existing files' style
            output += f"{field}: [{', '.join(items)}]\n"
        else:
            val_str = str(val).replace('"', '')
            output += f"{field}: \"{val_str}\"\n"
    if len(output) > 0:
        output = "<!--\n" + output + "-->"

    whole_file = Path(path_to_file).read_text()

    if whole_file.startswith("<!--"):
        body = whole_file.split("-->", 1)[1]
    else:
        body = whole_file

    Path(path_to_file).write_text(output + body)


def read_metadata(meta):
    metadata_dictionary = {}
    pattern = r"((<!--|---)\n)((.|\n)*?)(\n(-->|---))"
    match_group = re.search(pattern, meta)

    # If metadata is found
    if match_group:
        raw_metadata = match_group[3]
        list_metadata = raw_metadata.split("\n")
        # Split the key: value pairs
        while list_metadata:
            line = list_metadata.pop(0)
            split_in_keywords = line.split(": ", 1)
            key = split_in_keywords[0]
            value = split_in_keywords[1]
            if verify_string_is_dictionary(value):
                value = unpack_dictionary_string_to_dictionary(
                    value)
            # If it's a multiline string
            while list_metadata and len(list_metadata[0].split(": ", 1)) <= 1:
                line = list_metadata.pop(0)
                value = value + line.lstrip(' ')
            value = value.strip("\"")
            metadata_dictionary[key] = value.lstrip('>-')

    return metadata_dictionary


def sanitize_page(path):
    """
    Converts the
        "<!--" -> "---"
        "-->" -> "---"
    It converts only the first occurrences of these patterns
    Side effect:
        If the document doesn't have purposeful metadata but it contains this pattern in it's body this function replace
        these patterns
    """

    body = Path(path).read_text()

    # Replace the metadata with comments
    body = body.replace("<!--", "---", 1)
    body = body.replace("-->", "---", 1)

    # Custom elements we don't want on GitHub
    body = body.replace("<!--unhideme", "")
    body = body.replace("unhideme-->", "")

    # MDX 3 compatibility replacements
    body = body.replace("<details><summary>", "<details>\n<summary>")
    body = body.replace("<details open><summary>", "<details open>\n<summary>")
    body = body.replace("${", r"$\{")
    body = body.replace("<=", r"\<=")
    body = body.replace("%<", r"%\<")
    body = body.replace("{{", r"\{\{")
    body = body.replace("style=\\{\\{", "style={{")
    body = body.replace("<->", r"\<->")
    body = body.replace("{attribute_name}", r"\{attribute_name}")
    body = body.replace("{attribute_unit}", r"\{attribute_unit}")

    # <url> into [url](url)
    body = re.sub(r'<(https://[^>]+)>', r'[\1](\1)', body)
    body = re.sub(r'<(http://[^>]+)>', r'[\1](\1)', body)
    body = re.sub(r'<([\w\.-]+@[\w\.-]+\.\w+)>', r'[\1](mailto:\1)', body)

    match_group = re.search(r'meta_yaml: "(.*)"', body)
    if match_group:
        # If the file has a meta_yaml field, then it is an integration, and we need to put the value into custom_edit_url too
        body = re.sub(r"meta_yaml:.*\n",
                      "",
                      re.sub(r'custom_edit_url:.*',
                             f"custom_edit_url: \"{match_group[1]}\"",
                             body))

    # The list with the lines that will be written in the file
    output = []

    # For each line of the file I read
    for line in body.splitlines():
        # If the line isn't an H1 title, and it isn't an analytics pixel, append it to the output list
        if not line.startswith("[![analytics]"):
            output.append(line + "\n")
    output = "".join(output)
    # Try to remove excess newlines from the start of the document
    output = re.sub(r'---(\n\s*\n)', '---\n\n', output)
    # Try to add a newline to the start of a document that has no newline
    if not re.match(r'---(\n\s*\n)', output):
        # print(path, "not matching")
        output = output.replace("---\n", "---\n\n", 2)
        # revert first line
        output = output.replace("---\n\n", "---\n", 1)

    # Open the file for overwriting, we are going to write the output list in the file
    Path(path).write_text(output)


def add_new_learn_path_key_to_dict(input_dict, docs_prefix, docs_path_learn, temp_folder):
    """
    This function takes as an argument our dictionary of the Ingest process and creates a new dictionary with key-value
    pairs of type Source file -> Target file (learn_absolute path)
    """
    output_dictionary = dict()
    for element in input_dict:
        repo = input_dict[element]["ingestedRepo"]
        file_path = element.replace(temp_folder + "/" + repo + "/", "")

        source_link = produce_gh_view_link_for_repo(repo, file_path)
        output_dictionary[source_link] = input_dict[element]["learnPath"] \
            .split(".mdx")[0] \
            .lstrip('"') \
            .rstrip('"') \
            .replace(docs_prefix, docs_path_learn)
        source_link = produce_gh_edit_link_for_repo(repo, file_path)
        output_dictionary[source_link] = input_dict[element]["learnPath"] \
            .split(".mdx")[0] \
            .lstrip('"') \
            .rstrip('"') \
            .replace(docs_prefix, docs_path_learn)

        # Check for pages that are category overview pages, and have filepath like ".../monitor/monitor".
        # This way we remove the double dirname in the end, because docusaurus routes the file to .../monitor
        if output_dictionary[source_link].split("/")[len(output_dictionary[source_link].split("/")) - 1] == \
                output_dictionary[source_link].split("/")[len(output_dictionary[source_link].split("/")) - 2]:
            same_parent_dir = output_dictionary[source_link].split(
                "/")[len(output_dictionary[source_link].split("/")) - 2]

            proper_link = output_dictionary[source_link].split(
                same_parent_dir, 1)
            output_dictionary[source_link] = proper_link[0] + \
                                             proper_link[1].strip("/")

        _temp = output_dictionary[source_link].replace("'", " ").replace(":", " ").replace(")", " ").replace(
            ",", " ").replace("(", " ").replace("/  +/g", ' ').replace(" ", "%20").replace('/-+/', '-')
        # If there is a slug present in the file, then that is the new_learn_path, with a "/docs" added in the front.
        try:
            input_dict[element].update(
                {"new_learn_path": "/docs" + input_dict[element]["metadata"]["slug"]})
        except KeyError:
            input_dict[element].update({"new_learn_path": _temp})

    return input_dict


def local_to_absolute_links(path_to_file, input_dict):
    """
    This function is able to recognize and parse relative github links like:

    ../../../../src/database/engine/README.md
    /docs/abc.md
    """

    whole_file = Path(path_to_file).read_text()

    global UNCORRELATED_LINK_COUNTER

    # Determine the repo this file belongs to
    source_repo = extract_repo_from_local_path(path_to_file)

    # Get the custom_edit_url from input_dict for this file (the GitHub URL to the source file)
    source_github_url = path_to_file  # fallback to local path
    try:
        source_github_url = input_dict[path_to_file]["metadata"]["custom_edit_url"]
    except (KeyError, TypeError):
        pass

    # Split the file into its metadata and body, so that this function doesn't touch the metadata fields
    # metadata = "---" + whole_file.split("---", 2)[1] + "---"
    body = whole_file

    # custom_edit_url_arr = re.findall(r'custom_edit_url(.*)', metadata)

    # print(input_dict.keys())

    # If there are links inside the body
    if re.search(r"\]\((.*?)\)", body):
        # Find all the links and add them in an array
        urls_with_headers = []
        temp = re.findall(r'\[\n|.*?]\((\n|.*?)\)', body)
        # For every link, store both the base URL and the header
        for link in temp:
            base_url = link.split('#')[0]
            header = link.split('#')[1] if '#' in link else ''
            urls_with_headers.append((base_url, header, link))

        # Deduplicate by full link
        seen = set()
        unique_urls = []
        for base_url, header, full_link in urls_with_headers:
            if full_link not in seen:
                seen.add(full_link)
                unique_urls.append((base_url, header, full_link))

        for url, header, full_link in unique_urls:
            # if not url.startswith("/"):

            if ".md" in url and (any(url in key for key in input_dict.keys())):
                if "http" not in url and url.startswith(".") and len(url) > 0:
                    # print("Link starts with '.'")
                    # The URL will get replaced by the value of the replaceString
                    # try:
                    url_to_replace = url
                    # print(url, path_to_file)

                    path_arr = path_to_file.split('/')

                    url_arr = url.split('/')

                    url_leftover = url_arr.copy()

                    path_arr.pop()
                    for piece in url_arr:
                        if piece == "..":
                            url_leftover.pop(0)
                            path_arr.pop()
                            print(path_arr)

                    replace = "/".join(path_arr) + "/" + "/".join(url_leftover)

                    check = Path(replace)

                    if check.exists():
                        body = body.replace(f"({url_to_replace}", "(" + replace)
                        # Validate header if present
                        if header and not validate_header_in_file(replace, header):
                            add_broken_header(source_repo, full_link, header, source_github_url)
                elif url.startswith("/"):
                    # print("link starting with dash")
                    url_to_replace = url

                    path_arr = path_to_file.split('/')

                    url_arr = url.split('/')

                    # print(path_arr, url_arr)

                    url_leftover = url_arr.copy()

                    replace = f"{path_arr[0]}/{path_arr[1]}" + "/".join(url_leftover)

                    check = Path(replace)

                    if check.exists():
                        body = body.replace(f"({url_to_replace}", "(" + replace)
                        # Validate header if present
                        if header and not validate_header_in_file(replace, header):
                            add_broken_header(source_repo, full_link, header, source_github_url)
            else:
                if (url.startswith(".") or url.startswith("/")):

                    directory = "ingest-temp-folder"
                    substring = url
                    found = False

                    for root, dirs, files in os.walk(directory):
                        for name in files + dirs:
                            if substring in os.path.join(root, name):
                                # print(url, path_to_file, path_to_file.split('/')[1], "NOT IN LEARN")

                                replace = f"(https://github.com/netdata/{path_to_file.split('/')[1]}/blob/master{url}"

                                body = body.replace(f"({url}", replace)
                                found = True
                                break
                        if found:
                            break

                    if not found:
                        UNCORRELATED_LINK_COUNTER += 1
                        add_broken_url(source_repo, url, source_github_url)

    Path(path_to_file).write_text(body)


def convert_github_links(path_to_file, input_dict):
    """
    Input:
        path: The path to the markdown file
        input_dict: the dictionary with every info about all files

    Expected format of links in files:
        [*](https://github.com/netdata/netdata/blob/master/*)
        or any other Netdata repo
    """

    whole_file = Path(path_to_file).read_text()

    global UNCORRELATED_LINK_COUNTER

    # Split the file into its metadata and body, so that this function doesn't touch the metadata fields
    metadata = "---" + whole_file.split("---", 2)[1] + "---"
    body = whole_file.split("---", 2)[2]

    custom_edit_url_arr = re.findall(r'custom_edit_url(.*)', metadata)

    # Determine the source repo from the custom_edit_url in metadata
    # This tells us which repo the current file originally came from
    source_repo = "unknown"
    source_github_url = path_to_file  # fallback to local path
    if custom_edit_url_arr and len(custom_edit_url_arr[0]) > 1:
        custom_edit_url = custom_edit_url_arr[0].replace("\"", "").strip(": ")
        source_repo = extract_repo_from_github_url(custom_edit_url)
        source_github_url = custom_edit_url

    # If there are links inside the body
    if re.search(r"\]\((.*?)\)", body):
        # Find all the links and add them in an array
        urls_with_headers = []
        temp = re.findall(r'\[\n|.*?]\((\n|.*?)\)', body)
        # For every link, store both the base URL and the header
        for link in temp:
            base_url = link.split('#')[0]
            header = link.split('#')[1] if '#' in link else ''
            urls_with_headers.append((base_url, header, link))

        for url, header, full_link in urls_with_headers:
            # The URL will get replaced by the value of the replaceString
            try:
                # The keys inside fileDict are like "ingest-temp-folder/netdata/collectors/charts.d.plugin/ap/README.md"
                # so from the link, we need:
                # 1. replace the https link prefix up until our organization identifier with the prefix of the temp folder
                # 2. try and catch any mishaps in links that instead of "blob" have "edit"
                # 3. remove "blob/master/" or "blob/main/"
                # 4. Then we have the correct key for the dictionary

                dict_key = url.replace("https://github.com/netdata", TEMP_FOLDER).replace(
                    "edit/", "blob/", 1).replace("blob/master/", "").replace("blob/main/", "")
                dictionary = input_dict[dict_key]

                replace_string = dictionary["new_learn_path"].replace("//", "/")

                # In some cases, a "id: someId" will be in a file, this is to change a file's link in Docusaurus,
                # so we need to be careful to honor that
                try:
                    metadata_id = dictionary["metadata"]["id"]

                    replace_string = replace_string.replace(
                        replace_string.split(
                            "/")[len(replace_string.split("/")) - 1],
                        metadata_id
                    )

                except Exception as e:
                    # There is no "id" metadata in the file, do nothing

                    pass

                body = body.replace("](" + url, "](" + replace_string)

                # Validate header if present - check against the source file in temp folder
                if header and dict_key in input_dict:
                    source_file = dict_key
                    if Path(source_file).exists() and not validate_header_in_file(source_file, header):
                        # Use source_repo (where the file with the broken link is from)
                        add_broken_header(source_repo, full_link, header, source_github_url)

                # In the end replace the URL with the replaceString
            except Exception as e:
                # This is probably a link that can't be translated to a Learn link (e.g. An external file)
                if url.startswith("https://github.com/netdata") and re.search(r"\.md", url):
                    # Try to rescue an integration link
                    if "integrations" in url and ("collector" in url):
                        # Due to the integrations/cloud_notifications/integrations/.. scenario, we use rsplit to remove the last occurrence of "integrations"
                        # We want to map links to specific integrations mds, to their parent README, in case the above try-catch failed to find the replacement.
                        try_url = url.rsplit("integrations", 1)[
                                      0] + "README.md"
                        # The URL will get replaced by the value of the replaceString
                        try:
                            # The keys inside fileDict are like "ingest-temp-folder/netdata/collectors/charts.d.plugin/ap/README.md"
                            # , so from the link, we need:
                            # replace the https link prefix until our organization identifier with the prefix of the temp folder
                            # try and catch any mishaps in links that instead of "blob" have "edit"
                            # remove "blob/master/" or "blob/main/"
                            # Then we have the correct key for the dictionary

                            dict_key = try_url.replace("https://github.com/netdata", TEMP_FOLDER).replace(
                                "edit", "blob").replace("blob/master/", "").replace("blob/main/", "")
                            dictionary = input_dict[dict_key]
                            replace_string = dictionary["new_learn_path"]

                            # In some cases, a "id: someId" will be in a file, this is to change a file's link in Docusaurus,
                            # so we need to be careful to honor that
                            try:
                                metadata_id = dictionary["metadata"]["id"]

                                replace_string = replace_string.replace(
                                    replace_string.split(
                                        "/")[len(replace_string.split("/")) - 1],
                                    metadata_id
                                )
                            except Exception as e:
                                # There is no "id" metadata in the file, do nothing
                                pass

                            # In the end replace the URL with the replaceString
                            body = body.replace("](" + url, "](" + replace_string)

                            # Validate header if present
                            if header and Path(dict_key).exists() and not validate_header_in_file(dict_key, header):
                                add_broken_header(source_repo, full_link, header, source_github_url)
                        except:
                            # Only mark as broken if the file doesn't exist in the repos
                            # Files that exist but aren't published to Learn are fine - they stay as GitHub links
                            if not file_exists_in_repos(url):
                                UNCORRELATED_LINK_COUNTER += 1
                                add_broken_url(source_repo, url, source_github_url)

                            if len(custom_edit_url_arr[0]) > 1:
                                custom_edit_url = custom_edit_url_arr[0].replace(
                                    "\"", "").strip(":")
                            else:
                                custom_edit_url = "NO custom_edit_url found, please add one"

                            # print(UNCORRELATED_LINK_COUNTER,
                            #       "INFO: In File:",
                            #       custom_edit_url,
                            #       "\n", "URL:", url, "\n")
                    else:
                        # Only mark as broken if the file doesn't exist in the repos
                        # Files that exist but aren't published to Learn are fine - they stay as GitHub links
                        if not file_exists_in_repos(url):
                            UNCORRELATED_LINK_COUNTER += 1
                            add_broken_url(source_repo, url, source_github_url)

    # Construct again the whole file
    whole_file = metadata + body

    # Write everything onto the file again
    Path(path_to_file).write_text(whole_file)


def automate_sidebar_position(dictionary):
    """
    Dynamically assigns position numbers to each file entry based on its depth in the directory.
    More room is provided between categories to accommodate up to 500 files per level.

    Levels:
    - Level 1: 100,000 gap between categories
    - Level 2: 2,000 gap between subcategories (allows room for 500+ files per category)
    - Level 3: 500 gap between sub-subcategories
    - Level 4: Serial numbering within Level 3 (increments by 10 for documents)
    """

    print("### Automating sidebar_position ###", '\n')

    position_array = []

    # Initial counters for each level
    counter_one = 100_000
    serial_two = 0  # Serial counter for level 2
    serial_three = 0  # Serial counter for level 3
    serial_four = 0  # Serial counter for level 4

    previous_levels = ["", "", ""]

    for path in dictionary['learn_rel_path']:
        if str(path) != "nan":
            split = str(path).split("/")

            # Ensure there are at least 3 levels (for category and subcategories)
            current_levels = split + [""] * (3 - len(split))

            # Update counters based on level changes
            if current_levels[0] != previous_levels[0]:
                # New Level 1 category, reset counters
                counter_one += 100_000
                serial_two = 0
                serial_three = 0
                serial_four = 0
            elif current_levels[1] != previous_levels[1]:
                # New Level 2 category, increment serial for Level 2
                serial_two += 2_000  # Allow 2,000 gap for each subcategory
                serial_three = 0  # Reset serial for Level 3
                serial_four = 0  # Reset serial for Level 4
            elif current_levels[2] != previous_levels[2]:
                # New Level 3 category, increment serial for Level 3
                serial_three += 500  # Allow 500 gap for files within each subcategory
                serial_four = 0  # Reset serial for Level 4
            else:
                # Increment serial for Level 4
                serial_four += 10  # Serial for documents within Level 3

            previous_levels = current_levels

            # Calculate the final position based on the counters
            position_value = counter_one + serial_two + serial_three + serial_four

            # Append the calculated position
            position_array.append(position_value)
        else:
            # If the path is nan, return -1
            position_array.append(-1)

    return position_array

def sort_files(file_array):
    most_popular = []
    rest_netdata_integrations = []
    community_integrations = []

    for file in file_array:
        if Path(file).is_file():
            # [filename, filepath, banner message, banner color]
            content = Path(file).read_text()

            if "most_popular: \"True\"" in content:
                most_popular.append(
                    [str(file).lower().rsplit("/", 1)[1], file, "by Netdata", "#00ab44"])
            elif "maintained%20by-Netdata-" in content:
                rest_netdata_integrations.append(
                    [str(file).lower().rsplit("/", 1)[1], file, "by Netdata", "#00ab44"])
            else:
                community_integrations.append(
                    [str(file).lower().rsplit("/", 1)[1], file, "by Community", "rgba(0, 0, 0, 0.25)"])

    sorted_array = sorted(
        most_popular) + sorted(rest_netdata_integrations) + sorted(community_integrations)

    return sorted_array


def get_dir_make_file_and_recurse(directory):
    dir_path, dir_name = str(directory).rsplit("/", 1)
    filename = f"{dir_path}/{dir_name}/{dir_name}.mdx"

    if any(Path(directory).glob("**/*")):
        sorted_list = sort_files(Path(directory).glob("**/*"))

        try:
            sidebar_position = re.search(
                r'sidebar_position:.*', Path(sorted_list[0][1]).read_text(encoding='utf-8'))[0]
        except (TypeError, IndexError):
            sidebar_position = ""
        

        sidebar_label = str(directory).rsplit("/", 1)[1]

        dir_path = Path(directory)
        try:
            # Compute path relative to the docs root (e.g. "docs/foo/bar" -> "foo/bar")
            relative_dir = dir_path.relative_to(Path("docs"))
            slug_path = clean_and_lower_string(str(relative_dir))
        except ValueError:
            # Fallback: use the full directory path if it is not under "docs"
            slug_path = clean_and_lower_string(str(dir_path))

        slug = "/" + slug_path.lstrip("/")

        md = \
            f"""---
sidebar_label: "{sidebar_label}"
{sidebar_position}
hide_table_of_contents: true
learn_status: "AUTOGENERATED"
slug: "{slug}"
learn_link: "https://learn.netdata.cloud/{clean_and_lower_string(str(directory))}"
---

# {sidebar_label}

import \u007b Grid, Box \u007d from '@site/src/components/Grid_integrations';

<Grid  columns="4">
"""

        integrations = 0
        direct_integrations = 0
        direct_non_integrations = 0

        for file_array_entry in sorted_list:
            file = file_array_entry[1]
            message = file_array_entry[2]
            color = file_array_entry[3]
            if Path(file).is_file():
                whole_file = Path(file).read_text(encoding='utf-8')

                direct_child = file.parent == Path(directory)

                if "DO NOT EDIT THIS FILE DIRECTLY" in whole_file:
                    if direct_child:
                        direct_integrations += 1

                    meta_dict = read_metadata(whole_file)

                    try:
                        img = re.search(r'<img src="https:\/\/netdata.cloud\/img.*', whole_file)[0].replace(
                            "width=\"150\"",
                            "style={{width: '90%', maxHeight: '100%', verticalAlign: 'middle' }}").replace("<img",
                                                                                                            "<img custom-image")
                    except TypeError:
                        img = ""

                    md += \
                        f"""
<Box banner="{message}" banner_color="{color}" to="{meta_dict["learn_link"].replace("https://learn.netdata.cloud", "")}"  title="{meta_dict["sidebar_label"]}">
{img}
</Box>
"""
                    integrations += 1
                else:
                    if direct_child:
                        direct_non_integrations += 1
        # if there are only integrations in current dir and subdirs
        # for the case where we got integrations in subdirs, but more files, like Collecting Metrics directory, we don't want to make anything
        if integrations > 0 and (
            (
                (direct_non_integrations <= direct_integrations and direct_integrations > 0)
                or (direct_non_integrations == 0 and direct_integrations == 0)
            )
            and (not Path(filename).is_file() and "docs.mdx" not in filename)
        ):
            md += "\n</Grid>"
            Path(filename).parent.mkdir(parents=True, exist_ok=True)
            Path(filename).write_text(md, encoding='utf-8')

        for subdir in sorted(Path(directory).glob("*/")):
            get_dir_make_file_and_recurse(subdir)

if __name__ == '__main__':
    parser = argparse.ArgumentParser(
        description='Ingest docs from multiple repositories')

    parser.add_argument(
        '--repos',
        default=[],
        nargs='+',
        help='Choose specific repo you want ingest, if not set, defaults ingested'
    )

    parser.add_argument(
        "--dry-run",
        help="Don't save a file with the output.",
        action="store_true",
    )

    parser.add_argument(
        "-d", "--debug",
        help="Enable debug printing",
        action="store_true",
    )

    parser.add_argument(
        "--docs-prefix",
        help="Don't save a file with the output.",
        dest="DOCS_PREFIX",
        default="docs"
    )

    parser.add_argument(
        "--fail-links",
        help="Exit with error code 1 at the end if any internal broken links are found.",
        action="store_true",
    )

    parser.add_argument(
        "--fail-links-netdata",
        help="Exit with error code 1 if broken links found in netdata repo.",
        action="store_true",
    )

    parser.add_argument(
        "--fail-links-helmchart",
        help="Exit with error code 1 if broken links found in helmchart repo.",
        action="store_true",
    )

    parser.add_argument(
        "--fail-links-onprem",
        help="Exit with error code 1 if broken links found in netdata-cloud-onprem repo.",
        action="store_true",
    )

    parser.add_argument(
        "--fail-links-asd",
        help="Exit with error code 1 if broken links found in agent-service-discovery repo.",
        action="store_true",
    )

    parser.add_argument(
        "--fail-links-grafana",
        help="Exit with error code 1 if broken links found in netdata-grafana-datasource-plugin repo.",
        action="store_true",
    )

    parser.add_argument(
        "--fail-links-github",
        help="Exit with error code 1 if broken links found in .github repo.",
        action="store_true",
    )

    parser.add_argument(
        "--gh-token",
        help="GitHub Personal Access Token for HTTPS cloning (optional, used only for local testing).",
        dest="gh_token",
        default=None,
    )

    parser.add_argument(
        "--use_plain_https",
        help="Clone using plain HTTPS without a token (overrides --gh-token if provided).",
        action="store_true",
        dest="use_plain_https",
    )

    parser.add_argument(
        "--ignore-on-prem-repo",
        help="Skip cloning the netdata-cloud-onprem repo and ignore broken links pointing to it.",
        action="store_true",
    )

    parser.add_argument(
        "--local-repo",
        dest="local_repos",
        default=[],
        nargs='+',
        help='Use a local directory instead of cloning. Format: repo_name:local_path (e.g., netdata:/path/to/netdata)'
    )

    list_of_repos_in_str = []
    # netdata/netdata:branch tkatsoulas/go.d.plugin:mybranch
    args = parser.parse_args()
    kArgs = args._get_kwargs()
    GITHUB_TOKEN = args.gh_token
    USE_PLAIN_HTTPS = args.use_plain_https

    # Create local copies from the parse_args input
    DOCS_PREFIX = args.DOCS_PREFIX
    for arg in kArgs:
        if arg[0] == "repos":
            list_of_repos_in_str = arg[1]
        if arg[0] == "dry_run":
            DRY_RUN = arg[1]
        if arg[0] == "debug" or arg[0] == "debug":
            if arg[1]:
                DEBUG = True
                print("RUNNING WITH DEBUG MESSAGES ON")
        if arg[0] == "fail_links":
            FAIL_ON_ALL_BROKEN_LINKS = arg[1]
        if arg[0] == "fail_links_netdata":
            if arg[1]:
                FAIL_ON_REPOS.add("netdata")
        if arg[0] == "fail_links_helmchart":
            if arg[1]:
                FAIL_ON_REPOS.add("helmchart")
        if arg[0] == "fail_links_onprem":
            if arg[1]:
                FAIL_ON_REPOS.add("netdata-cloud-onprem")
        if arg[0] == "fail_links_asd":
            if arg[1]:
                FAIL_ON_REPOS.add("agent-service-discovery")
        if arg[0] == "fail_links_grafana":
            if arg[1]:
                FAIL_ON_REPOS.add("netdata-grafana-datasource-plugin")
        if arg[0] == "fail_links_github":
            if arg[1]:
                FAIL_ON_REPOS.add(".github")
        if arg[0] == "ignore_on_prem_repo":
            if arg[1]:
                IGNORE_ON_PREM_REPO = True
                print(
                    "IGNORING ON-PREM REPO: Will skip cloning and ignore broken links pointing to netdata-cloud-onprem")
        if arg[0] == "local_repos":
            for local_repo_str in arg[1]:
                try:
                    repo_name, local_path = local_repo_str.split(":", 1)
                    if not os.path.isdir(local_path):
                        print(
                            f"ERROR: Local path '{local_path}' for repo '{repo_name}' does not exist or is not a directory")
                        exit(-1)
                    LOCAL_REPOS[repo_name] = os.path.abspath(local_path)
                    print(f"Using local directory for {repo_name}: {LOCAL_REPOS[repo_name]}")
                except ValueError:
                    print(
                        f"ERROR: Invalid --local-repo format '{local_repo_str}'. Expected format: repo_name:local_path")
                    exit(-1)

    if len(list_of_repos_in_str) > 0:
        for repo_str in list_of_repos_in_str:
            try:
                _temp = repo_str.split("/")
                repo_owner, repository, repo_branch = [
                                                          _temp[0]] + (_temp[1].split(":"))
                default_repos[repository]["owner"] = repo_owner
                default_repos[repository]["branch"] = repo_branch
            except (TypeError, ValueError):
                print(
                    "You specified a wrong format in at least one of the repos you want to ingest")
                parser.print_usage()
                exit(-1)
            except KeyError:
                print(repository)
                print("The repo you specified in not in predefined repos")
                print(default_repos.keys())
                parser.print_usage()
                exit(-1)
            except Exception as exc:
                print("Unknown error in parsing", exc)

    USE_PLAIN_HTTPS = USE_PLAIN_HTTPS or IGNORE_ON_PREM_REPO

    # Clean up old clones into a temp dir
    unsafe_cleanup_folders(TEMP_FOLDER)
    # Clean up old ingested docs
    safe_cleanup_learn_folders(DOCS_PREFIX)
    print("Creating a temp directory: ", TEMP_FOLDER)

    try:
        os.mkdir(TEMP_FOLDER)
    except FileExistsError:
        print("Folder already exists")

    # Clone all the predefined repos
    for repo_name in default_repos.keys():
        # Skip on-prem repo if flag is set
        if IGNORE_ON_PREM_REPO and repo_name == "netdata-cloud-onprem":
            print(f"Skipping {repo_name} (--ignore-on-prem-repo flag is set)")
            continue
        # Use local directory if specified, otherwise clone
        if repo_name in LOCAL_REPOS:
            local_path = LOCAL_REPOS[repo_name]
            dest_path = os.path.join(TEMP_FOLDER, repo_name)
            print(f"Copying local directory {local_path} to {dest_path}")
            shutil.copytree(local_path, dest_path, symlinks=True)
            print(f"Using local directory for {repo_name} (copied from {local_path})")
        else:
            print(clone_repo(
                default_repos[repo_name]["owner"],
                repo_name,
                default_repos[repo_name]["branch"],
                1,
                TEMP_FOLDER + "/",
                GITHUB_TOKEN,
                USE_PLAIN_HTTPS,
            ))

    shutil.move("ingest-temp-folder/netdata/docs/.map/map.csv", "map.csv")

    # We fetch the markdown files from the repositories
    all_markdown_files = fetch_markdown_from_repo(TEMP_FOLDER)
    print("Files detected: ", len(all_markdown_files), "\n")

    # Fill the mapDict with the metadata the integration mds have (autogenerated metadata)
    mapDict = populate_integrations(all_markdown_files)

    # set the index to the unique custom_edit_url column
    mapDict.set_index('custom_edit_url').T.to_dict('dict')

    # Automate the sidebar position
    mapDict['sidebar_position'] = automate_sidebar_position(mapDict)
    # Make the column type integer
    mapDict['sidebar_position'] = mapDict['sidebar_position'].astype(int)

    markdown_files_with_metadata = []

    for markdown in all_markdown_files:
        # print("File: ", markdown)
        md_metadata = insert_and_read_hidden_metadata_from_doc(
            markdown, mapDict)
        # Check to see if the dictionary returned is empty
        if len(md_metadata) > 0:
            # print("FOUND METADATA", markdown)
            # print(metadata)
            markdown_files_with_metadata.append(markdown)
            if "learn_status" in md_metadata.keys() and md_metadata["learn_status"] == "Published":
                try:
                    # check the type of the response (for more info of what the response can be check
                    # the return statements of the function itself)
                    response = create_mdx_path_from_metadata(md_metadata)
                    sanitize_regex = r'`|\(|\)'
                    to_publish[markdown] = {
                        "metadata": md_metadata,
                        "learnPath": str(response[0]),
                        "ingestedRepo": str(markdown.split("/", 2)[1])
                    }

                    md_metadata.update({"learn_link": "https://learn.netdata.cloud/docs" + response[1], "slug": response[1]})

                    update_metadata_of_file(markdown, md_metadata)
                except KeyError as exc:
                    print(
                        f"File {markdown} doesn't contain key-value", exc)
            else:
                # We don't need these files
                rest_files_with_metadata_dictionary[markdown] = {
                    "metadata": md_metadata,
                    "learnPath": str(f"docs/_archive/_{markdown}"),
                    "ingestedRepo": str(markdown.split("/", 2)[1])
                }
        # Don't fail on empty markdown
        elif not os.stat(markdown).st_size == 0:
            rest_files_dictionary[markdown] = {"tmpPath": markdown}
        del md_metadata

    # FILE MOVING
    print("### Moving files ###\n")

    # identify published documents
    print(f"### Found Learn files: {len(to_publish)}###\n")

    for md_file in to_publish:
        local_to_absolute_links(md_file, to_publish)
        copy_doc(md_file, to_publish[md_file]["learnPath"])
        sanitize_page(to_publish[md_file]["learnPath"])

    print("### Fixing github links ###")

    # After the moving, we have a new metadata, called new_learn_path, and we utilize that to fix links that were
    # pointing to GitHub relative paths
    file_dict = add_new_learn_path_key_to_dict(
        to_publish, DOCS_PREFIX, "/docs", TEMP_FOLDER)

    for md_file in to_publish:
        convert_github_links(file_dict[md_file]["learnPath"], file_dict)

    genRedirects.main(file_dict)
    print("Done.", "Uncorrelated links (links from our github repos that the files are not in Learn):",
          UNCORRELATED_LINK_COUNTER)

    # Print deduplicated list of uncorrelated URLs grouped by repo
    total_broken_urls = sum(len(urls) for urls in UNCORRELATED_URLS_BY_REPO.values())
    repos_with_broken_urls = set()
    if total_broken_urls > 0:
        print("\n### Uncorrelated URLs (grouped by repo) ###")
        for repo in sorted(UNCORRELATED_URLS_BY_REPO.keys()):
            repo_urls = UNCORRELATED_URLS_BY_REPO[repo]
            if len(repo_urls) > 0:
                repos_with_broken_urls.add(repo)
                print(f"\n=== Repo: {repo} ({len(repo_urls)} broken URLs) ===")
                for url in sorted(repo_urls.keys()):
                    print(f"\n  Broken link: {url}")
                    print(f"  Referenced in {len(repo_urls[url])} file(s):")
                    for source_file in sorted(repo_urls[url]):
                        print(f"    - {source_file}")
        print(f"\nTotal unique broken URLs: {total_broken_urls}")

    # Print broken header links grouped by repo
    total_broken_headers = sum(len(headers) for headers in BROKEN_HEADER_LINKS_BY_REPO.values())
    repos_with_broken_headers = set()
    if total_broken_headers > 0:
        print("\n### Broken Header/Anchor Links (grouped by repo) ###")
        for repo in sorted(BROKEN_HEADER_LINKS_BY_REPO.keys()):
            repo_headers = BROKEN_HEADER_LINKS_BY_REPO[repo]
            if len(repo_headers) > 0:
                repos_with_broken_headers.add(repo)
                print(f"\n=== Repo: {repo} ({len(repo_headers)} broken header links) ===")
                for (full_link, header) in sorted(repo_headers.keys()):
                    print(f"\n  Broken link: {full_link}")
                    print(f"  Missing header: #{header}")
                    print(f"  Referenced in {len(repo_headers[(full_link, header)])} file(s):")
                    for source_file in sorted(repo_headers[(full_link, header)]):
                        print(f"    - {source_file}")
        print(f"\nTotal unique broken header links: {total_broken_headers}")

    # Combine all repos with any broken links
    all_repos_with_issues = repos_with_broken_urls | repos_with_broken_headers

    # Determine if we should fail based on flags
    should_fail = False
    failed_repos = []

    # Check if --fail is set (fail on any broken link)
    if FAIL_ON_ALL_BROKEN_LINKS and len(all_repos_with_issues) > 0:
        should_fail = True
        failed_repos = list(all_repos_with_issues)

    # Check per-repo failure flags
    for repo in FAIL_ON_REPOS:
        if repo in all_repos_with_issues:
            should_fail = True
            if repo not in failed_repos:
                failed_repos.append(repo)

    # Store failure state but continue processing
    SHOULD_EXIT_WITH_FAILURE = False
    if should_fail:
        SHOULD_EXIT_WITH_FAILURE = True
        print(f"\n### BROKEN LINKS DETECTED in repos: {', '.join(sorted(failed_repos))} ###")
        print("Will exit with error code 1 at the end due to flags:")
        if FAIL_ON_ALL_BROKEN_LINKS:
            print("  --fail-links")
        for repo in sorted(failed_repos):
            if repo in FAIL_ON_REPOS:
                # Map repo names to flag names
                flag_map = {
                    "netdata": "--fail-links-netdata",
                    "helmchart": "--fail-links-helmchart",
                    "netdata-cloud-onprem": "--fail-links-onprem",
                    "agent-service-discovery": "--fail-links-asd",
                    "netdata-grafana-datasource-plugin": "--fail-links-grafana",
                    ".github": "--fail-links-github"
                }
                print(f"  {flag_map.get(repo, repo)}")
        print("\nContinuing with remaining ingest steps...")

    if DEBUG:
        # Print the list of markdown not in Learn, for debugging purposes
        if len(rest_files_dictionary):
            print("ABORT: Files found that are not in the map, exiting...")
            for md_file in rest_files_dictionary:
                print(rest_files_dictionary[md_file]["tmpPath"])

    # Write the current dict into a file, so we can check for redirects in the next commit
    temp_dict = {}
    custom_edit_urls_array = []
    new_learn_paths_array = []

    for repo_name in file_dict:
        custom_edit_urls_array.append(
            file_dict[repo_name]["metadata"]["custom_edit_url"])
        new_learn_paths_array.append(file_dict[repo_name]["new_learn_path"])

    temp_dict['custom_edit_url'] = custom_edit_urls_array
    temp_dict['learn_path'] = new_learn_paths_array

    df = pd.DataFrame.from_dict(temp_dict)
    df.set_index('custom_edit_url')
    df.to_csv("./ingest/one_commit_back_file-dict.tsv", sep='\t', index=False)

    unsafe_cleanup_folders(TEMP_FOLDER)
    os.remove("map.csv")

    # check if we need integration grid
    get_dir_make_file_and_recurse(f"./{DOCS_PREFIX}")
    ensure_category_json_for_dirs(DOCS_PREFIX)

    # Print final operation status with exit code
    if SHOULD_EXIT_WITH_FAILURE:
        print("\n" + "=" * 60)
        print("OPERATION FINISHED - FAILURE (exit code: 1)")
        print("Broken links detected matching failure criteria")
        print("=" * 60)
    else:
        print("\n" + "=" * 60)
        print("OPERATION FINISHED - SUCCESS (exit code: 0)")
        print("=" * 60)

    # Exit with failure if broken links were detected and failure flag was set
    if SHOULD_EXIT_WITH_FAILURE:
        exit(1)
